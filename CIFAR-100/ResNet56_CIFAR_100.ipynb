{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet56_CIFAR-100.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtNz2s/u+bH0YdpVNdDDNX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnrghks5583/Way-that-set-threshold-for-setting-PMD/blob/main/CIFAR-100/ResNet56_CIFAR_100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf-s4M28Jb5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bf8c9d-4abd-4576-fa01-c0a717429ec2"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, Input, Add, ReLU, AveragePooling2D, GlobalAveragePooling2D, ZeroPadding2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "num_classes = 100\n",
        "\n",
        "# load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-1f4y65JmUc",
        "outputId": "57a47f8c-ac57-4151-cb31-b8c39c966b7c"
      },
      "source": [
        "base_model = ResNet50(include_top = False, weights = 'imagenet')\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "shortcut = x\n",
        "shortcut = Conv2D(filters = 2048, kernel_size = 1, strides = 1)(shortcut)\n",
        "shortcut = BatchNormalization(axis = 3, epsilon=1.001e-5)(shortcut)\n",
        "\n",
        "x = Conv2D(filters = 512, kernel_size = 1, strides = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 1024, kernel_size = 3, padding='SAME')(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5,)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 2048, kernel_size = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "\n",
        "x = Add()([shortcut, x])\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "shortcut = x\n",
        "shortcut = Conv2D(filters = 2048, kernel_size = 1, strides = 1)(shortcut)\n",
        "shortcut = BatchNormalization(axis = 3, epsilon=1.001e-5)(shortcut)\n",
        "\n",
        "x = Conv2D(filters = 512, kernel_size = 1, strides = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 1024, kernel_size = 3, padding='SAME')(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5,)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 2048, kernel_size = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "\n",
        "x = Add()([shortcut, x])\n",
        "x = Activation('relu')(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "prediction = Dense(num_classes, activation='softmax')(x)\n",
        "model = Model(inputs = base_model.input, outputs = prediction)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, None, None, 6 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, None, None, 6 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, None, None, 6 0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, None, None, 6 4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, None, None, 6 256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, None, None, 6 0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, None, None, 6 256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, None, None, 6 0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, None, None, 2 16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, None, None, 2 0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, None, None, 2 0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, None, None, 6 256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, None, None, 6 0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, None, None, 6 256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, None, None, 6 0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, None, None, 2 0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, None, None, 2 0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, None, None, 6 256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, None, None, 6 0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, None, None, 6 256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, None, None, 6 0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, None, None, 2 0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, None, None, 2 0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, None, None, 1 32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, None, None, 1 512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, None, None, 1 0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, None, None, 1 512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, None, None, 1 0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, None, None, 5 131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, None, None, 5 0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, None, None, 5 0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, None, None, 1 512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, None, None, 1 0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, None, None, 1 512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, None, None, 1 0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, None, None, 5 0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, None, None, 5 0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, None, None, 1 512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, None, None, 1 0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, None, None, 1 512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, None, None, 1 0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, None, None, 5 0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, None, None, 5 0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, None, None, 1 512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, None, None, 1 0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, None, None, 1 512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, None, None, 1 0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, None, None, 5 0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, None, None, 5 0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, None, None, 2 131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, None, None, 2 0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, None, None, 2 0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, None, None, 1 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, None, None, 1 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, None, None, 1 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, None, None, 2 0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, None, None, 2 0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, None, None, 1 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, None, None, 1 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, None, None, 2 0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, None, None, 2 0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, None, None, 1 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, None, None, 1 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, None, None, 2 0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, None, None, 2 0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, None, None, 1 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, None, None, 1 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, None, None, 2 0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, None, None, 2 0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, None, None, 1 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, None, None, 1 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, None, None, 2 0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, None, None, 2 0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, None, None, 1 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, None, None, 1 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, None, None, 5 524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, None, None, 5 0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, None, None, 5 0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, None, None, 2 2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, None, None, 2 0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, None, None, 2 0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, None, None, 5 0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, None, None, 5 0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, None, None, 2 0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, None, None, 2 0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, None, None, 5 0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, None, None, 5 0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, None, None, 2 0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, None, None, 2 0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 5 1049088     conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 5 2048        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, None, None, 5 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 1 4719616     activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 1 4096        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None, 1 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 2 4196352     conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 2 2099200     activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 2 8192        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 2 8192        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, None, 2 0           batch_normalization[0][0]        \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, None, 2 0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 5 1049088     activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 5 2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, None, 5 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 1 4719616     activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 1 4096        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 2 4196352     activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 2 2099200     activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 2 8192        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 2 8192        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, None, 2 0           batch_normalization_4[0][0]      \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, None, None, 2 0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          204900      global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 47,966,180\n",
            "Trainable params: 47,890,532\n",
            "Non-trainable params: 75,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIQf9lP6Joce"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [lr_reducer, lr_scheduler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7S_pO1NJqep",
        "outputId": "ffd40ed3-abd6-412e-fcf7-50d9d79f101c"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, validation_split = 0.1, batch_size = 128, epochs = 100, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "Epoch 1/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 76s 106ms/step - loss: 4.2675 - accuracy: 0.0960 - val_loss: 22.8968 - val_accuracy: 0.0890\n",
            "Epoch 2/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 3.9551 - accuracy: 0.1359 - val_loss: 400.3135 - val_accuracy: 0.0822\n",
            "Epoch 3/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 3.4119 - accuracy: 0.2119 - val_loss: 110.1341 - val_accuracy: 0.0846\n",
            "Epoch 4/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 3.0651 - accuracy: 0.2620 - val_loss: 15.4198 - val_accuracy: 0.1208\n",
            "Epoch 5/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 2.7571 - accuracy: 0.3193 - val_loss: 3.2101 - val_accuracy: 0.2752\n",
            "Epoch 6/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 2.6309 - accuracy: 0.3483 - val_loss: 4.6164 - val_accuracy: 0.1436\n",
            "Epoch 7/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 2.4324 - accuracy: 0.3741 - val_loss: 2.8620 - val_accuracy: 0.2862\n",
            "Epoch 8/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 2.2092 - accuracy: 0.4202 - val_loss: 43.3740 - val_accuracy: 0.0414\n",
            "Epoch 9/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 1.9982 - accuracy: 0.4637 - val_loss: 5.6526 - val_accuracy: 0.1216\n",
            "Epoch 10/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 1.7740 - accuracy: 0.5098 - val_loss: 2.4936 - val_accuracy: 0.3830\n",
            "Epoch 11/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 1.5713 - accuracy: 0.5610 - val_loss: 2.7219 - val_accuracy: 0.3980\n",
            "Epoch 12/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 1.4685 - accuracy: 0.5812 - val_loss: 2.7357 - val_accuracy: 0.3640\n",
            "Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 1.2602 - accuracy: 0.6341 - val_loss: 2.4738 - val_accuracy: 0.4102\n",
            "Epoch 14/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 1.0752 - accuracy: 0.6815 - val_loss: 6.6424 - val_accuracy: 0.4048\n",
            "Epoch 15/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.9225 - accuracy: 0.7186 - val_loss: 3.5350 - val_accuracy: 0.3432\n",
            "Epoch 16/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.7851 - accuracy: 0.7593 - val_loss: 3.1950 - val_accuracy: 0.3618\n",
            "Epoch 17/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.6876 - accuracy: 0.7810 - val_loss: 3.5828 - val_accuracy: 0.3808\n",
            "Epoch 18/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.5784 - accuracy: 0.8182 - val_loss: 3.4194 - val_accuracy: 0.4082\n",
            "Epoch 19/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.5040 - accuracy: 0.8397 - val_loss: 3.6866 - val_accuracy: 0.3814\n",
            "Epoch 20/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 0.4502 - accuracy: 0.8568 - val_loss: 3.5101 - val_accuracy: 0.4036\n",
            "Epoch 21/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.4108 - accuracy: 0.8677 - val_loss: 4.1007 - val_accuracy: 0.3930\n",
            "Epoch 22/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.3423 - accuracy: 0.8881 - val_loss: 4.2782 - val_accuracy: 0.3620\n",
            "Epoch 23/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3241 - accuracy: 0.8971 - val_loss: 4.0165 - val_accuracy: 0.4012\n",
            "Epoch 24/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.3087 - accuracy: 0.9022 - val_loss: 4.7737 - val_accuracy: 0.3440\n",
            "Epoch 25/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.3053 - accuracy: 0.9028 - val_loss: 4.2691 - val_accuracy: 0.3750\n",
            "Epoch 26/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.4109 - accuracy: 0.8728 - val_loss: 4.2666 - val_accuracy: 0.3868\n",
            "Epoch 27/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.4096 - accuracy: 0.8747 - val_loss: 4.3396 - val_accuracy: 0.3778\n",
            "Epoch 28/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.4509 - accuracy: 0.8626 - val_loss: 3.7014 - val_accuracy: 0.3882\n",
            "Epoch 29/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3924 - accuracy: 0.8804 - val_loss: 5.3871 - val_accuracy: 0.3290\n",
            "Epoch 30/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.2665 - accuracy: 0.9182 - val_loss: 4.8718 - val_accuracy: 0.3490\n",
            "Epoch 31/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3000 - accuracy: 0.9086 - val_loss: 24.6295 - val_accuracy: 0.1316\n",
            "Epoch 32/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.4020 - accuracy: 0.8798 - val_loss: 4.2219 - val_accuracy: 0.4042\n",
            "Epoch 33/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3050 - accuracy: 0.9072 - val_loss: 4.3860 - val_accuracy: 0.3778\n",
            "Epoch 34/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.5875 - accuracy: 0.8300 - val_loss: 4.4725 - val_accuracy: 0.3222\n",
            "Epoch 35/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3223 - accuracy: 0.9026 - val_loss: 4.9193 - val_accuracy: 0.3366\n",
            "Epoch 36/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3000 - accuracy: 0.9093 - val_loss: 4.0611 - val_accuracy: 0.4154\n",
            "Epoch 37/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.4092 - accuracy: 0.8770 - val_loss: 4.4531 - val_accuracy: 0.3082\n",
            "Epoch 38/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.3521 - accuracy: 0.8942 - val_loss: 4.2109 - val_accuracy: 0.3892\n",
            "Epoch 39/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1684 - accuracy: 0.9481 - val_loss: 4.7662 - val_accuracy: 0.3884\n",
            "Epoch 40/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1348 - accuracy: 0.9588 - val_loss: 4.3943 - val_accuracy: 0.4252\n",
            "Epoch 41/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1111 - accuracy: 0.9661 - val_loss: 5.2388 - val_accuracy: 0.3980\n",
            "Epoch 42/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.5207 - accuracy: 0.8481 - val_loss: 3.9296 - val_accuracy: 0.4020\n",
            "Epoch 43/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.3195 - accuracy: 0.9027 - val_loss: 5.6545 - val_accuracy: 0.4082\n",
            "Epoch 44/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.2158 - accuracy: 0.9348 - val_loss: 9.8011 - val_accuracy: 0.3474\n",
            "Epoch 45/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1510 - accuracy: 0.9546 - val_loss: 4.6561 - val_accuracy: 0.3948\n",
            "Epoch 46/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1540 - accuracy: 0.9528 - val_loss: 5.0139 - val_accuracy: 0.3828\n",
            "Epoch 47/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1999 - accuracy: 0.9406 - val_loss: 6.4446 - val_accuracy: 0.2992\n",
            "Epoch 48/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1914 - accuracy: 0.9439 - val_loss: 4.3942 - val_accuracy: 0.4090\n",
            "Epoch 49/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1528 - accuracy: 0.9534 - val_loss: 5.1341 - val_accuracy: 0.4044\n",
            "Epoch 50/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1198 - accuracy: 0.9638 - val_loss: 5.6835 - val_accuracy: 0.3644\n",
            "Epoch 51/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1877 - accuracy: 0.9425 - val_loss: 6.0790 - val_accuracy: 0.4062\n",
            "Epoch 52/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1171 - accuracy: 0.9646 - val_loss: 4.6071 - val_accuracy: 0.4176\n",
            "Epoch 53/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1923 - accuracy: 0.9420 - val_loss: 6.2559 - val_accuracy: 0.3676\n",
            "Epoch 54/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.2938 - accuracy: 0.9148 - val_loss: 4.3167 - val_accuracy: 0.4032\n",
            "Epoch 55/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1584 - accuracy: 0.9521 - val_loss: 4.3876 - val_accuracy: 0.4204\n",
            "Epoch 56/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1236 - accuracy: 0.9622 - val_loss: 6.0517 - val_accuracy: 0.3244\n",
            "Epoch 57/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1926 - accuracy: 0.9421 - val_loss: 5.4574 - val_accuracy: 0.3650\n",
            "Epoch 58/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1557 - accuracy: 0.9532 - val_loss: 4.5490 - val_accuracy: 0.4144\n",
            "Epoch 59/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1823 - accuracy: 0.9454 - val_loss: 5.6745 - val_accuracy: 0.3672\n",
            "Epoch 60/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.2034 - accuracy: 0.9376 - val_loss: 4.5790 - val_accuracy: 0.3980\n",
            "Epoch 61/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1104 - accuracy: 0.9666 - val_loss: 7.1121 - val_accuracy: 0.2952\n",
            "Epoch 62/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1142 - accuracy: 0.9642 - val_loss: 4.6231 - val_accuracy: 0.4122\n",
            "Epoch 63/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1089 - accuracy: 0.9660 - val_loss: 4.7409 - val_accuracy: 0.4196\n",
            "Epoch 64/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.0916 - accuracy: 0.9714 - val_loss: 4.8693 - val_accuracy: 0.4170\n",
            "Epoch 65/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1119 - accuracy: 0.9649 - val_loss: 5.0465 - val_accuracy: 0.4148\n",
            "Epoch 66/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1069 - accuracy: 0.9670 - val_loss: 4.8229 - val_accuracy: 0.3826\n",
            "Epoch 67/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1310 - accuracy: 0.9594 - val_loss: 4.6473 - val_accuracy: 0.4152\n",
            "Epoch 68/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1059 - accuracy: 0.9668 - val_loss: 5.8782 - val_accuracy: 0.3352\n",
            "Epoch 69/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1594 - accuracy: 0.9525 - val_loss: 4.1792 - val_accuracy: 0.3910\n",
            "Epoch 70/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.1446 - accuracy: 0.9562 - val_loss: 4.9566 - val_accuracy: 0.3746\n",
            "Epoch 71/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.1763 - accuracy: 0.9471 - val_loss: 4.5334 - val_accuracy: 0.3956\n",
            "Epoch 72/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 33s 95ms/step - loss: 0.2846 - accuracy: 0.9161 - val_loss: 6.4381 - val_accuracy: 0.4010\n",
            "Epoch 73/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.1141 - accuracy: 0.9659 - val_loss: 5.1916 - val_accuracy: 0.4112\n",
            "Epoch 74/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0735 - accuracy: 0.9772 - val_loss: 8.8538 - val_accuracy: 0.3852\n",
            "Epoch 75/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0924 - accuracy: 0.9716 - val_loss: 10.8967 - val_accuracy: 0.1160\n",
            "Epoch 76/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.3112 - accuracy: 0.9088 - val_loss: 4.6209 - val_accuracy: 0.4004\n",
            "Epoch 77/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0959 - accuracy: 0.9706 - val_loss: 7.8053 - val_accuracy: 0.3810\n",
            "Epoch 78/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0793 - accuracy: 0.9754 - val_loss: 5.0835 - val_accuracy: 0.4260\n",
            "Epoch 79/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 5.2174 - val_accuracy: 0.3846\n",
            "Epoch 80/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0679 - accuracy: 0.9796 - val_loss: 5.5088 - val_accuracy: 0.4190\n",
            "Epoch 81/100\n",
            "Learning rate:  0.001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 0.0612 - accuracy: 0.9813 - val_loss: 7.1156 - val_accuracy: 0.3896\n",
            "Epoch 82/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0454 - accuracy: 0.9863 - val_loss: 4.9446 - val_accuracy: 0.4326\n",
            "Epoch 83/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0263 - accuracy: 0.9925 - val_loss: 5.0575 - val_accuracy: 0.4332\n",
            "Epoch 84/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0227 - accuracy: 0.9933 - val_loss: 5.4331 - val_accuracy: 0.4264\n",
            "Epoch 85/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 5.3420 - val_accuracy: 0.4362\n",
            "Epoch 86/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.0181 - accuracy: 0.9947 - val_loss: 5.7875 - val_accuracy: 0.4352\n",
            "Epoch 87/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.0158 - accuracy: 0.9956 - val_loss: 5.7401 - val_accuracy: 0.4344\n",
            "Epoch 88/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 5.6955 - val_accuracy: 0.4356\n",
            "Epoch 89/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 5.7263 - val_accuracy: 0.4374\n",
            "Epoch 90/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 5.7455 - val_accuracy: 0.4410\n",
            "Epoch 91/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 7.4700 - val_accuracy: 0.4224\n",
            "Epoch 92/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 6.2971 - val_accuracy: 0.4388\n",
            "Epoch 93/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 6.6907 - val_accuracy: 0.4258\n",
            "Epoch 94/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0130 - accuracy: 0.9963 - val_loss: 6.3263 - val_accuracy: 0.4370\n",
            "Epoch 95/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 95ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 7.0492 - val_accuracy: 0.4366\n",
            "Epoch 96/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 6.3807 - val_accuracy: 0.4334\n",
            "Epoch 97/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 96ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 6.6430 - val_accuracy: 0.4358\n",
            "Epoch 98/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 6.2016 - val_accuracy: 0.4406\n",
            "Epoch 99/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 6.4496 - val_accuracy: 0.4372\n",
            "Epoch 100/100\n",
            "Learning rate:  0.0001\n",
            "352/352 [==============================] - 34s 97ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 6.3771 - val_accuracy: 0.4382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efdc7d90190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLRUqhPvJttB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ba090b-3c4c-4295-cd1f-27ee7bc2e74a"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 7s 20ms/step - loss: 6.9260 - accuracy: 0.4518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.925996780395508, 0.45179998874664307]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}