{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet56_CALTECH-101.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjvXROTzPK/Dvjt9KiMdO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnrghks5583/Way-that-set-threshold-for-setting-PMD/blob/main/CALTECH-101/ResNet56_CALTECH_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8FGSX3ySWSg",
        "outputId": "402ad36a-2d8d-41c1-eebe-58f3e9a0b519"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "image_size = (164, 164)\n",
        "input_shape = (164, 164, 3)\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/ResNet_졸과/GP/pickle_101_164'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70OBbmkKSvF8"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, Input, Add, ReLU, AveragePooling2D, GlobalAveragePooling2D, ZeroPadding2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "\n",
        "image_size = (164, 164)\n",
        "input_shape = (164, 164, 3)\n",
        "num_classes = 102"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP-U14IaS9XP"
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "with gzip.open('caltech101_x_trainData.pickle', 'rb') as f:\n",
        "    x_train = pickle.load(f)\n",
        "    \n",
        "with gzip.open('caltech101_x_testData.pickle', 'rb') as f:\n",
        "    x_test = pickle.load(f)\n",
        "    \n",
        "with gzip.open('caltech101_y_trainData.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "    \n",
        "with gzip.open('caltech101_y_testData.pickle', 'rb') as f:\n",
        "    y_test = pickle.load(f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoZG0z8hS_48",
        "outputId": "9921f0a6-c07a-483b-8236-eb32476c0097"
      },
      "source": [
        "\n",
        "base_model = ResNet50(include_top = False, weights = 'imagenet')\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "shortcut = x\n",
        "shortcut = Conv2D(filters = 2048, kernel_size = 1, strides = 1)(shortcut)\n",
        "shortcut = BatchNormalization(axis = 3, epsilon=1.001e-5)(shortcut)\n",
        "\n",
        "x = Conv2D(filters = 512, kernel_size = 1, strides = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 1024, kernel_size = 3, padding='SAME')(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5,)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 2048, kernel_size = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "\n",
        "x = Add()([shortcut, x])\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "shortcut = x\n",
        "shortcut = Conv2D(filters = 2048, kernel_size = 1, strides = 1)(shortcut)\n",
        "shortcut = BatchNormalization(axis = 3, epsilon=1.001e-5)(shortcut)\n",
        "\n",
        "x = Conv2D(filters = 512, kernel_size = 1, strides = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 1024, kernel_size = 3, padding='SAME')(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5,)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(filters = 2048, kernel_size = 1)(x)\n",
        "x = BatchNormalization(axis = 3, epsilon=1.001e-5)(x)\n",
        "\n",
        "x = Add()([shortcut, x])\n",
        "x = Activation('relu')(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "prediction = Dense(num_classes, activation='softmax')(x)\n",
        "model = Model(inputs = base_model.input, outputs = prediction)\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, None, None, 6 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, None, None, 6 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, None, None, 6 0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, None, None, 6 4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, None, None, 6 256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, None, None, 6 0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, None, None, 6 256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, None, None, 6 0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, None, None, 2 16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, None, None, 2 0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, None, None, 2 0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, None, None, 6 256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, None, None, 6 0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, None, None, 6 256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, None, None, 6 0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, None, None, 2 0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, None, None, 2 0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, None, None, 6 256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, None, None, 6 0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, None, None, 6 256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, None, None, 6 0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, None, None, 2 0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, None, None, 2 0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, None, None, 1 32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, None, None, 1 512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, None, None, 1 0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, None, None, 1 512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, None, None, 1 0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, None, None, 5 131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, None, None, 5 0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, None, None, 5 0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, None, None, 1 512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, None, None, 1 0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, None, None, 1 512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, None, None, 1 0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, None, None, 5 0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, None, None, 5 0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, None, None, 1 512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, None, None, 1 0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, None, None, 1 512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, None, None, 1 0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, None, None, 5 0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, None, None, 5 0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, None, None, 1 512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, None, None, 1 0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, None, None, 1 512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, None, None, 1 0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, None, None, 5 0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, None, None, 5 0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, None, None, 2 131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, None, None, 2 0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, None, None, 2 0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, None, None, 1 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, None, None, 1 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, None, None, 1 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, None, None, 2 0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, None, None, 2 0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, None, None, 1 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, None, None, 1 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, None, None, 2 0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, None, None, 2 0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, None, None, 1 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, None, None, 1 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, None, None, 2 0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, None, None, 2 0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, None, None, 1 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, None, None, 1 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, None, None, 2 0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, None, None, 2 0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, None, None, 1 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, None, None, 1 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, None, None, 2 0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, None, None, 2 0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, None, None, 1 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, None, None, 1 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, None, None, 5 524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, None, None, 5 0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, None, None, 5 0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, None, None, 2 2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, None, None, 2 0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, None, None, 2 0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, None, None, 5 0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, None, None, 5 0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, None, None, 2 0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, None, None, 2 0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, None, None, 5 0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, None, None, 5 0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, None, None, 2 0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, None, None, 2 0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 5 1049088     conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 5 2048        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, None, None, 5 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 1 4719616     activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 1 4096        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None, 1 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 2 4196352     conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 2 2099200     activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 2 8192        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 2 8192        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, None, 2 0           batch_normalization[0][0]        \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, None, 2 0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 5 1049088     activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 5 2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, None, 5 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 1 4719616     activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 1 4096        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 2 4196352     activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 2 2099200     activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 2 8192        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 2 8192        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, None, 2 0           batch_normalization_4[0][0]      \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, None, None, 2 0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 102)          208998      global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 47,970,278\n",
            "Trainable params: 47,894,630\n",
            "Non-trainable params: 75,648\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJw2JTF3YbCb"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sarnV1p3TC3Q",
        "outputId": "a631e2da-e542-4d9a-a7cb-082843daac17"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [lr_reducer, lr_scheduler]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCaWp_3oTGSg",
        "outputId": "c9916469-c6e7-4c09-c453-5ffda25df8f6"
      },
      "source": [
        "model.fit(x_train, y_train, validation_split = 0.1, batch_size = 32, epochs = 100, callbacks=callbacks)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 95s 279ms/step - loss: 2.9614 - accuracy: 0.3799 - val_loss: 5.1254 - val_accuracy: 0.1749\n",
            "Epoch 2/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 55s 269ms/step - loss: 1.7224 - accuracy: 0.5639 - val_loss: 2.2151 - val_accuracy: 0.5137\n",
            "Epoch 3/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 57s 275ms/step - loss: 1.0260 - accuracy: 0.7198 - val_loss: 1.6496 - val_accuracy: 0.6120\n",
            "Epoch 4/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 57s 278ms/step - loss: 0.6305 - accuracy: 0.8193 - val_loss: 1.2634 - val_accuracy: 0.7063\n",
            "Epoch 5/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 279ms/step - loss: 0.4143 - accuracy: 0.8782 - val_loss: 1.8483 - val_accuracy: 0.5943\n",
            "Epoch 6/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.2954 - accuracy: 0.9128 - val_loss: 1.7438 - val_accuracy: 0.5820\n",
            "Epoch 7/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.2045 - accuracy: 0.9392 - val_loss: 2.1855 - val_accuracy: 0.6189\n",
            "Epoch 8/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.2408 - accuracy: 0.9294 - val_loss: 1.2316 - val_accuracy: 0.7309\n",
            "Epoch 9/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.1106 - accuracy: 0.9670 - val_loss: 1.9796 - val_accuracy: 0.6202\n",
            "Epoch 10/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.1549 - accuracy: 0.9525 - val_loss: 1.6659 - val_accuracy: 0.6544\n",
            "Epoch 11/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.2077 - accuracy: 0.9380 - val_loss: 1.9445 - val_accuracy: 0.6284\n",
            "Epoch 12/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.1184 - accuracy: 0.9648 - val_loss: 2.1594 - val_accuracy: 0.5929\n",
            "Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.1088 - accuracy: 0.9651 - val_loss: 2.1225 - val_accuracy: 0.6544\n",
            "Epoch 14/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0780 - accuracy: 0.9754 - val_loss: 1.2406 - val_accuracy: 0.7445\n",
            "Epoch 15/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 0.1278 - accuracy: 0.9625 - val_loss: 1.7270 - val_accuracy: 0.6694\n",
            "Epoch 16/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 0.0978 - accuracy: 0.9692 - val_loss: 1.7908 - val_accuracy: 0.7063\n",
            "Epoch 17/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0830 - accuracy: 0.9765 - val_loss: 1.8788 - val_accuracy: 0.6762\n",
            "Epoch 18/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 279ms/step - loss: 0.0741 - accuracy: 0.9777 - val_loss: 1.6602 - val_accuracy: 0.7336\n",
            "Epoch 19/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0731 - accuracy: 0.9806 - val_loss: 1.5681 - val_accuracy: 0.6817\n",
            "Epoch 20/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0612 - accuracy: 0.9824 - val_loss: 1.7665 - val_accuracy: 0.6557\n",
            "Epoch 21/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0791 - accuracy: 0.9784 - val_loss: 4.0441 - val_accuracy: 0.5246\n",
            "Epoch 22/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 279ms/step - loss: 0.1004 - accuracy: 0.9719 - val_loss: 1.9565 - val_accuracy: 0.6708\n",
            "Epoch 23/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.1094 - accuracy: 0.9687 - val_loss: 3.1486 - val_accuracy: 0.5437\n",
            "Epoch 24/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 57s 279ms/step - loss: 0.0955 - accuracy: 0.9725 - val_loss: 1.9199 - val_accuracy: 0.6694\n",
            "Epoch 25/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0731 - accuracy: 0.9783 - val_loss: 1.5016 - val_accuracy: 0.7063\n",
            "Epoch 26/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0327 - accuracy: 0.9909 - val_loss: 1.3750 - val_accuracy: 0.7473\n",
            "Epoch 27/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0190 - accuracy: 0.9951 - val_loss: 1.0394 - val_accuracy: 0.8033\n",
            "Epoch 28/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0245 - accuracy: 0.9938 - val_loss: 1.6988 - val_accuracy: 0.7418\n",
            "Epoch 29/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0597 - accuracy: 0.9830 - val_loss: 1.4886 - val_accuracy: 0.7131\n",
            "Epoch 30/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.1091 - accuracy: 0.9646 - val_loss: 2.3318 - val_accuracy: 0.6503\n",
            "Epoch 31/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.1141 - accuracy: 0.9680 - val_loss: 1.8851 - val_accuracy: 0.6872\n",
            "Epoch 32/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0797 - accuracy: 0.9769 - val_loss: 1.7452 - val_accuracy: 0.6680\n",
            "Epoch 33/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 1.2391 - val_accuracy: 0.7623\n",
            "Epoch 34/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0190 - accuracy: 0.9956 - val_loss: 1.0684 - val_accuracy: 0.7896\n",
            "Epoch 35/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0124 - accuracy: 0.9974 - val_loss: 1.6850 - val_accuracy: 0.7404\n",
            "Epoch 36/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0517 - accuracy: 0.9839 - val_loss: 1.8569 - val_accuracy: 0.6885\n",
            "Epoch 37/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0513 - accuracy: 0.9866 - val_loss: 1.7600 - val_accuracy: 0.7295\n",
            "Epoch 38/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0524 - accuracy: 0.9847 - val_loss: 2.3614 - val_accuracy: 0.6175\n",
            "Epoch 39/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0680 - accuracy: 0.9796 - val_loss: 1.7567 - val_accuracy: 0.7131\n",
            "Epoch 40/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0358 - accuracy: 0.9901 - val_loss: 1.8775 - val_accuracy: 0.6899\n",
            "Epoch 41/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0666 - accuracy: 0.9810 - val_loss: 1.6084 - val_accuracy: 0.7309\n",
            "Epoch 42/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0447 - accuracy: 0.9857 - val_loss: 1.5741 - val_accuracy: 0.7186\n",
            "Epoch 43/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0369 - accuracy: 0.9892 - val_loss: 1.2983 - val_accuracy: 0.7705\n",
            "Epoch 44/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0581 - accuracy: 0.9836 - val_loss: 2.1235 - val_accuracy: 0.6134\n",
            "Epoch 45/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0522 - accuracy: 0.9845 - val_loss: 1.5429 - val_accuracy: 0.7445\n",
            "Epoch 46/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0511 - accuracy: 0.9872 - val_loss: 1.9348 - val_accuracy: 0.6735\n",
            "Epoch 47/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 1.3937 - val_accuracy: 0.7623\n",
            "Epoch 48/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 1.0439 - val_accuracy: 0.8019\n",
            "Epoch 49/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 1.1214 - val_accuracy: 0.7855\n",
            "Epoch 50/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0090 - accuracy: 0.9974 - val_loss: 2.1821 - val_accuracy: 0.6598\n",
            "Epoch 51/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 280ms/step - loss: 0.0744 - accuracy: 0.9810 - val_loss: 2.1336 - val_accuracy: 0.6448\n",
            "Epoch 52/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 0.0503 - accuracy: 0.9868 - val_loss: 1.5891 - val_accuracy: 0.7541\n",
            "Epoch 53/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 281ms/step - loss: 0.0207 - accuracy: 0.9945 - val_loss: 1.2987 - val_accuracy: 0.7787\n",
            "Epoch 54/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0264 - accuracy: 0.9935 - val_loss: 1.7397 - val_accuracy: 0.7036\n",
            "Epoch 55/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0592 - accuracy: 0.9831 - val_loss: 1.6286 - val_accuracy: 0.7227\n",
            "Epoch 56/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0572 - accuracy: 0.9869 - val_loss: 1.7498 - val_accuracy: 0.7049\n",
            "Epoch 57/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0367 - accuracy: 0.9901 - val_loss: 1.1460 - val_accuracy: 0.8005\n",
            "Epoch 58/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0215 - accuracy: 0.9945 - val_loss: 1.2519 - val_accuracy: 0.7732\n",
            "Epoch 59/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0124 - accuracy: 0.9971 - val_loss: 1.2558 - val_accuracy: 0.7719\n",
            "Epoch 60/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 59s 285ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 1.3413 - val_accuracy: 0.7637\n",
            "Epoch 61/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0450 - accuracy: 0.9869 - val_loss: 1.5404 - val_accuracy: 0.7568\n",
            "Epoch 62/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 2.3174 - val_accuracy: 0.6612\n",
            "Epoch 63/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0369 - accuracy: 0.9886 - val_loss: 1.3603 - val_accuracy: 0.7623\n",
            "Epoch 64/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 1.3903 - val_accuracy: 0.7828\n",
            "Epoch 65/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 1.1385 - val_accuracy: 0.8019\n",
            "Epoch 66/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 1.2219 - val_accuracy: 0.8046\n",
            "Epoch 67/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 8.6991e-04 - accuracy: 0.9998 - val_loss: 1.1890 - val_accuracy: 0.8101\n",
            "Epoch 68/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 5.9658e-04 - accuracy: 0.9997 - val_loss: 1.1736 - val_accuracy: 0.8060\n",
            "Epoch 69/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 4.6540e-04 - accuracy: 0.9998 - val_loss: 1.1598 - val_accuracy: 0.8046\n",
            "Epoch 70/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 4.8372e-04 - accuracy: 0.9997 - val_loss: 1.1613 - val_accuracy: 0.8074\n",
            "Epoch 71/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 5.1254e-04 - accuracy: 0.9997 - val_loss: 1.1581 - val_accuracy: 0.8115\n",
            "Epoch 72/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 3.8971e-04 - accuracy: 0.9998 - val_loss: 1.1609 - val_accuracy: 0.8142\n",
            "Epoch 73/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 4.1246e-04 - accuracy: 0.9998 - val_loss: 1.1655 - val_accuracy: 0.8087\n",
            "Epoch 74/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 4.1769e-04 - accuracy: 0.9998 - val_loss: 1.1756 - val_accuracy: 0.8101\n",
            "Epoch 75/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 4.1105e-04 - accuracy: 0.9997 - val_loss: 1.1555 - val_accuracy: 0.8115\n",
            "Epoch 76/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 3.6962e-04 - accuracy: 0.9997 - val_loss: 1.1643 - val_accuracy: 0.8128\n",
            "Epoch 77/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 3.5278e-04 - accuracy: 0.9997 - val_loss: 1.1625 - val_accuracy: 0.8115\n",
            "Epoch 78/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 2.6551e-04 - accuracy: 0.9998 - val_loss: 1.1722 - val_accuracy: 0.8115\n",
            "Epoch 79/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 3.4397e-04 - accuracy: 0.9998 - val_loss: 1.1704 - val_accuracy: 0.8156\n",
            "Epoch 80/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 4.8813e-04 - accuracy: 0.9998 - val_loss: 1.1828 - val_accuracy: 0.8142\n",
            "Epoch 81/100\n",
            "Learning rate:  0.001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.1619 - val_accuracy: 0.8169\n",
            "Epoch 82/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 3.7948e-04 - accuracy: 0.9998 - val_loss: 1.1445 - val_accuracy: 0.8169\n",
            "Epoch 83/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.9735e-04 - accuracy: 0.9998 - val_loss: 1.1444 - val_accuracy: 0.8183\n",
            "Epoch 84/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 3.0867e-04 - accuracy: 0.9998 - val_loss: 1.1412 - val_accuracy: 0.8183\n",
            "Epoch 85/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.7372e-04 - accuracy: 0.9998 - val_loss: 1.1445 - val_accuracy: 0.8169\n",
            "Epoch 86/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 2.7269e-04 - accuracy: 0.9998 - val_loss: 1.1454 - val_accuracy: 0.8169\n",
            "Epoch 87/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.6616e-04 - accuracy: 0.9998 - val_loss: 1.1481 - val_accuracy: 0.8169\n",
            "Epoch 88/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 59s 285ms/step - loss: 2.9006e-04 - accuracy: 0.9997 - val_loss: 1.1456 - val_accuracy: 0.8169\n",
            "Epoch 89/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.4712e-04 - accuracy: 0.9998 - val_loss: 1.1469 - val_accuracy: 0.8169\n",
            "Epoch 90/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.7975e-04 - accuracy: 0.9997 - val_loss: 1.1369 - val_accuracy: 0.8156\n",
            "Epoch 91/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.8095e-04 - accuracy: 0.9997 - val_loss: 1.1435 - val_accuracy: 0.8142\n",
            "Epoch 92/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.8453e-04 - accuracy: 0.9998 - val_loss: 1.1440 - val_accuracy: 0.8142\n",
            "Epoch 93/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.5733e-04 - accuracy: 0.9998 - val_loss: 1.1482 - val_accuracy: 0.8156\n",
            "Epoch 94/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.5925e-04 - accuracy: 0.9997 - val_loss: 1.1506 - val_accuracy: 0.8142\n",
            "Epoch 95/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.7103e-04 - accuracy: 0.9997 - val_loss: 1.1472 - val_accuracy: 0.8142\n",
            "Epoch 96/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 2.4527e-04 - accuracy: 0.9998 - val_loss: 1.1498 - val_accuracy: 0.8156\n",
            "Epoch 97/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 2.6586e-04 - accuracy: 0.9997 - val_loss: 1.1475 - val_accuracy: 0.8156\n",
            "Epoch 98/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 283ms/step - loss: 3.5040e-04 - accuracy: 0.9998 - val_loss: 1.3069 - val_accuracy: 0.8033\n",
            "Epoch 99/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 59s 286ms/step - loss: 2.6149e-04 - accuracy: 0.9998 - val_loss: 1.1692 - val_accuracy: 0.8156\n",
            "Epoch 100/100\n",
            "Learning rate:  0.0001\n",
            "206/206 [==============================] - 58s 282ms/step - loss: 2.4718e-04 - accuracy: 0.9998 - val_loss: 1.1661 - val_accuracy: 0.8183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f85962e1a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z_3oK-pTHtj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb35dbb6-18d0-4bde-8724-f43f719e70f8"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 5s 94ms/step - loss: 0.8956 - accuracy: 0.8403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8956156373023987, 0.8403499126434326]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}