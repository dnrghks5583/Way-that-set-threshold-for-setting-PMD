{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Continue_ResNet_CALTECH-256.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1-OPBVoyDEp4uRKfTT_7mp1JXnF7l7oMI",
      "authorship_tag": "ABX9TyP+c5U59vM7eTw1huJwv02X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnrghks5583/Way-that-set-threshold-for-setting-PMD/blob/main/CALTECH-256/Continue_ResNet_CALTECH_256.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd1UXOA-3WYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc4d1c1-fcf9-4fdf-cce6-ec830353f2d9"
      },
      "source": [
        "from google.colab import drive\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#load most recent saved model\n",
        "directory_name = 'CALTECH-256_no_augmentation_models_with_lr_e-3'\n",
        "learning_rate_variable = int(directory_name[-1])\n",
        "image_size = (128, 128)\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/ResNet_졸과/GP/pickle_256'\n",
        "\n",
        "os.chdir(path)\n",
        "\n",
        "\n",
        "directory_path = path+'/' + directory_name + '/'\n",
        "file_name_and_time_lst = []\n",
        "\n",
        "for saved_models in os.listdir(f\"{directory_path}\"):\n",
        "    written_time = os.path.getctime(f\"{directory_path}{saved_models}\")\n",
        "    file_name_and_time_lst.append((saved_models, written_time))\n",
        " \n",
        "sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
        "latest_model = sorted_file_lst[0][0] #originally [0][0]\n",
        "print('\\nLoaded model name : ' , latest_model)\n",
        "\n",
        "#pickup from the last epoch\n",
        "initial_epoch = int(latest_model[len(latest_model)-6:-3])\n",
        "\n",
        "model = tf.keras.models.load_model(directory_path + latest_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Loaded model name :  CALTECH-256_ResNet56v1_model.085.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVCXMOwInJJi"
      },
      "source": [
        "\"\"\"\n",
        "ResNet v1\n",
        "[a] Deep Residual Learning for Image Recognition\n",
        "https://arxiv.org/pdf/1512.03385.pdf\n",
        "ResNet v2\n",
        "[b] Identity Mappings in Deep Residual Networks\n",
        "https://arxiv.org/pdf/1603.05027.pdf\n",
        "\n",
        "https://github.com/dnrghks5583/Way-that-set-threshold-for-setting-PMD/blob/main/Caltech101/caltech101_resnet.ipynb\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input\n",
        "from tensorflow.keras.layers import Flatten, add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# training parameters\n",
        "batch_size = 128 # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "num_classes = 257\n",
        "data_augmentation = True\n",
        "\n",
        "# subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 9\n",
        "\n",
        "# model version\n",
        "# orig paper: version = 1 (ResNet v1), \n",
        "# improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE9u4slNWpa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f8e80093-b710-491c-dd2f-096ccc4caa9a"
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "with gzip.open('caltech256_x_trainData.pickle', 'rb') as f:\n",
        "    x_train = pickle.load(f)\n",
        "    \n",
        "with gzip.open('caltech256_x_testData.pickle', 'rb') as f:\n",
        "    x_test = pickle.load(f)\n",
        "    \n",
        "with gzip.open('caltech256_y_trainData.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "    \n",
        "with gzip.open('caltech256_y_testData.pickle', 'rb') as f:\n",
        "    y_test = pickle.load(f)\n",
        "\n",
        "\"\"\"\n",
        "# score for previous trained model\n",
        "scores = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])  \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# score for previous trained model\\nscores = model.evaluate(x_test,\\n                        y_test,\\n                        batch_size=batch_size,\\n                        verbose=0)\\n\\nprint('Test loss:', scores[0])\\nprint('Test accuracy:', scores[1])  \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99-uDudWsIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5404412-cc9e-4595-e385-29144896e89c"
      },
      "source": [
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# if subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('Model_Type : ' , model_type)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "# convert class vectors to binary class matrices.\n",
        "#y_train = to_categorical(y_train, num_classes)\n",
        "#y_test = to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model_Type :  ResNet56v1\n",
            "x_train shape: (24485, 128, 128, 3)\n",
            "24485 train samples\n",
            "6122 test samples\n",
            "y_train shape: (24485, 257)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAsujpy2WsxH"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1*(10**(-learning_rate_variable)) #originally 3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5*(10**(-learning_rate_variable)) #originally 3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1*(10**(-learning_rate_variable)) #originally 3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1*(10**(-learning_rate_variable+1)) #originally 2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1*(10**(-learning_rate_variable+2)) #originally 1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUuM0NbMWxaJ"
      },
      "source": [
        "# prepare model model saving directory.\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), directory_name)\n",
        "model_name = 'CALTECH-256_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "                             \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alla_hLjW3Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8751fc8c-3d05-430e-e9a3-20a6c7689444"
      },
      "source": [
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "#callbacks = [lr_reducer, lr_scheduler]\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # this will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sBqraTKW5EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2051e5-9d57-46d7-cc3f-2589002f4a48"
      },
      "source": [
        "    # compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    steps_per_epoch =  math.ceil(len(x_train) / batch_size)\n",
        "    # fit the model on the batches generated by datagen.flow().\n",
        "    model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "              initial_epoch = initial_epoch,\n",
        "              verbose=1,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              callbacks=callbacks)\n",
        "\n",
        "\n",
        "# score trained model\n",
        "scores = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 86/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 125s 634ms/step - loss: 0.5322 - acc: 0.9261 - val_loss: 3.7734 - val_acc: 0.5124\n",
            "\n",
            "Epoch 00086: val_acc improved from -inf to 0.51241, saving model to /content/gdrive/My Drive/Colab Notebooks/ResNet_졸과/GP/pickle_256/CALTECH-256_no_augmentation_models_with_lr_e-3/CALTECH-256_ResNet56v1_model.086.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 87/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.5218 - acc: 0.9280 - val_loss: 3.8482 - val_acc: 0.5111\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.51241\n",
            "Epoch 88/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 629ms/step - loss: 0.5146 - acc: 0.9309 - val_loss: 3.9753 - val_acc: 0.5091\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.51241\n",
            "Epoch 89/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4996 - acc: 0.9355 - val_loss: 3.8986 - val_acc: 0.5126\n",
            "\n",
            "Epoch 00089: val_acc improved from 0.51241 to 0.51258, saving model to /content/gdrive/My Drive/Colab Notebooks/ResNet_졸과/GP/pickle_256/CALTECH-256_no_augmentation_models_with_lr_e-3/CALTECH-256_ResNet56v1_model.089.h5\n",
            "Epoch 90/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 629ms/step - loss: 0.4983 - acc: 0.9358 - val_loss: 3.8962 - val_acc: 0.5127\n",
            "\n",
            "Epoch 00090: val_acc improved from 0.51258 to 0.51274, saving model to /content/gdrive/My Drive/Colab Notebooks/ResNet_졸과/GP/pickle_256/CALTECH-256_no_augmentation_models_with_lr_e-3/CALTECH-256_ResNet56v1_model.090.h5\n",
            "Epoch 91/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4987 - acc: 0.9341 - val_loss: 3.9180 - val_acc: 0.5109\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.51274\n",
            "Epoch 92/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 629ms/step - loss: 0.4875 - acc: 0.9380 - val_loss: 3.9482 - val_acc: 0.5095\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.51274\n",
            "Epoch 93/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 629ms/step - loss: 0.4880 - acc: 0.9378 - val_loss: 4.0031 - val_acc: 0.5136\n",
            "\n",
            "Epoch 00093: val_acc improved from 0.51274 to 0.51356, saving model to /content/gdrive/My Drive/Colab Notebooks/ResNet_졸과/GP/pickle_256/CALTECH-256_no_augmentation_models_with_lr_e-3/CALTECH-256_ResNet56v1_model.093.h5\n",
            "Epoch 94/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 629ms/step - loss: 0.4809 - acc: 0.9371 - val_loss: 4.1518 - val_acc: 0.5082\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.51356\n",
            "Epoch 95/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4678 - acc: 0.9440 - val_loss: 3.9427 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.51356\n",
            "Epoch 96/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4672 - acc: 0.9416 - val_loss: 4.1453 - val_acc: 0.5069\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.51356\n",
            "Epoch 97/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4605 - acc: 0.9433 - val_loss: 3.9413 - val_acc: 0.5113\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.51356\n",
            "Epoch 98/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4583 - acc: 0.9429 - val_loss: 4.3160 - val_acc: 0.5114\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.51356\n",
            "Epoch 99/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4520 - acc: 0.9444 - val_loss: 4.2264 - val_acc: 0.5093\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.51356\n",
            "Epoch 100/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4493 - acc: 0.9462 - val_loss: 4.0669 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.51356\n",
            "Epoch 101/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4377 - acc: 0.9466 - val_loss: 4.2981 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.51356\n",
            "Epoch 102/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4423 - acc: 0.9460 - val_loss: 4.3370 - val_acc: 0.5088\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.51356\n",
            "Epoch 103/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4336 - acc: 0.9490 - val_loss: 4.1905 - val_acc: 0.5098\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.51356\n",
            "Epoch 104/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4263 - acc: 0.9515 - val_loss: 4.2516 - val_acc: 0.5087\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.51356\n",
            "Epoch 105/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4386 - acc: 0.9471 - val_loss: 4.1989 - val_acc: 0.5096\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.51356\n",
            "Epoch 106/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4261 - acc: 0.9493 - val_loss: 4.1845 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.51356\n",
            "Epoch 107/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4209 - acc: 0.9527 - val_loss: 4.2913 - val_acc: 0.5072\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.51356\n",
            "Epoch 108/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4169 - acc: 0.9514 - val_loss: 4.4598 - val_acc: 0.5091\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.51356\n",
            "Epoch 109/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4142 - acc: 0.9517 - val_loss: 4.2583 - val_acc: 0.5074\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.51356\n",
            "Epoch 110/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4138 - acc: 0.9518 - val_loss: 4.2644 - val_acc: 0.5121\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.51356\n",
            "Epoch 111/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4047 - acc: 0.9539 - val_loss: 4.3093 - val_acc: 0.5126\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.51356\n",
            "Epoch 112/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4076 - acc: 0.9534 - val_loss: 4.3098 - val_acc: 0.5091\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.51356\n",
            "Epoch 113/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.4003 - acc: 0.9539 - val_loss: 4.1773 - val_acc: 0.5121\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.51356\n",
            "Epoch 114/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3999 - acc: 0.9539 - val_loss: 4.3240 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.51356\n",
            "Epoch 115/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3972 - acc: 0.9548 - val_loss: 4.2334 - val_acc: 0.5087\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.51356\n",
            "Epoch 116/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3917 - acc: 0.9576 - val_loss: 4.1333 - val_acc: 0.5101\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.51356\n",
            "Epoch 117/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3874 - acc: 0.9575 - val_loss: 4.3845 - val_acc: 0.5113\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.51356\n",
            "Epoch 118/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3882 - acc: 0.9562 - val_loss: 4.4381 - val_acc: 0.5088\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.51356\n",
            "Epoch 119/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3814 - acc: 0.9575 - val_loss: 4.2801 - val_acc: 0.5072\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.51356\n",
            "Epoch 120/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3794 - acc: 0.9588 - val_loss: 4.4717 - val_acc: 0.5060\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.51356\n",
            "Epoch 121/200\n",
            "Learning rate:  0.0001\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3840 - acc: 0.9561 - val_loss: 4.2421 - val_acc: 0.5105\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.51356\n",
            "Epoch 122/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3708 - acc: 0.9618 - val_loss: 4.4074 - val_acc: 0.5123\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.51356\n",
            "Epoch 123/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3615 - acc: 0.9660 - val_loss: 4.3855 - val_acc: 0.5108\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.51356\n",
            "Epoch 124/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3537 - acc: 0.9690 - val_loss: 4.3808 - val_acc: 0.5124\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.51356\n",
            "Epoch 125/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3580 - acc: 0.9654 - val_loss: 4.3979 - val_acc: 0.5123\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.51356\n",
            "Epoch 126/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3510 - acc: 0.9694 - val_loss: 4.3969 - val_acc: 0.5144\n",
            "\n",
            "Epoch 00126: val_acc improved from 0.51356 to 0.51437, saving model to /content/gdrive/My Drive/Colab Notebooks/ResNet_졸과/GP/pickle_256/CALTECH-256_no_augmentation_models_with_lr_e-3/CALTECH-256_ResNet56v1_model.126.h5\n",
            "Epoch 127/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3565 - acc: 0.9653 - val_loss: 4.3618 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.51437\n",
            "Epoch 128/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3583 - acc: 0.9648 - val_loss: 4.4786 - val_acc: 0.5116\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.51437\n",
            "Epoch 129/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3540 - acc: 0.9663 - val_loss: 4.3840 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.51437\n",
            "Epoch 130/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3499 - acc: 0.9679 - val_loss: 4.4060 - val_acc: 0.5134\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.51437\n",
            "Epoch 131/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3546 - acc: 0.9672 - val_loss: 4.3869 - val_acc: 0.5140\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.51437\n",
            "Epoch 132/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3526 - acc: 0.9675 - val_loss: 4.4605 - val_acc: 0.5168\n",
            "\n",
            "Epoch 00132: val_acc improved from 0.51437 to 0.51682, saving model to /content/gdrive/My Drive/Colab Notebooks/ResNet_졸과/GP/pickle_256/CALTECH-256_no_augmentation_models_with_lr_e-3/CALTECH-256_ResNet56v1_model.132.h5\n",
            "Epoch 133/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 627ms/step - loss: 0.3568 - acc: 0.9659 - val_loss: 4.4121 - val_acc: 0.5149\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.51682\n",
            "Epoch 134/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3517 - acc: 0.9679 - val_loss: 4.4411 - val_acc: 0.5121\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.51682\n",
            "Epoch 135/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3479 - acc: 0.9682 - val_loss: 4.4037 - val_acc: 0.5136\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.51682\n",
            "Epoch 136/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3504 - acc: 0.9679 - val_loss: 4.4087 - val_acc: 0.5139\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.51682\n",
            "Epoch 137/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3501 - acc: 0.9677 - val_loss: 4.4902 - val_acc: 0.5140\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.51682\n",
            "Epoch 138/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3504 - acc: 0.9685 - val_loss: 4.3775 - val_acc: 0.5136\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.51682\n",
            "Epoch 139/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3480 - acc: 0.9676 - val_loss: 4.4134 - val_acc: 0.5157\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.51682\n",
            "Epoch 140/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3469 - acc: 0.9685 - val_loss: 4.4753 - val_acc: 0.5157\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.51682\n",
            "Epoch 141/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3408 - acc: 0.9703 - val_loss: 4.4528 - val_acc: 0.5140\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.51682\n",
            "Epoch 142/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3463 - acc: 0.9677 - val_loss: 4.4490 - val_acc: 0.5144\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.51682\n",
            "Epoch 143/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3441 - acc: 0.9695 - val_loss: 4.4095 - val_acc: 0.5136\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.51682\n",
            "Epoch 144/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3425 - acc: 0.9695 - val_loss: 4.4199 - val_acc: 0.5157\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.51682\n",
            "Epoch 145/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3453 - acc: 0.9693 - val_loss: 4.5065 - val_acc: 0.5134\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.51682\n",
            "Epoch 146/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3450 - acc: 0.9688 - val_loss: 4.5439 - val_acc: 0.5147\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.51682\n",
            "Epoch 147/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3473 - acc: 0.9695 - val_loss: 4.5028 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.51682\n",
            "Epoch 148/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3436 - acc: 0.9690 - val_loss: 4.4419 - val_acc: 0.5136\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.51682\n",
            "Epoch 149/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3407 - acc: 0.9710 - val_loss: 4.4809 - val_acc: 0.5136\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.51682\n",
            "Epoch 150/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3443 - acc: 0.9684 - val_loss: 4.3872 - val_acc: 0.5142\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.51682\n",
            "Epoch 151/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 627ms/step - loss: 0.3436 - acc: 0.9695 - val_loss: 4.4214 - val_acc: 0.5145\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.51682\n",
            "Epoch 152/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3428 - acc: 0.9692 - val_loss: 4.4748 - val_acc: 0.5154\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.51682\n",
            "Epoch 153/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3453 - acc: 0.9703 - val_loss: 4.4600 - val_acc: 0.5150\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.51682\n",
            "Epoch 154/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3404 - acc: 0.9705 - val_loss: 4.4816 - val_acc: 0.5155\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.51682\n",
            "Epoch 155/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3402 - acc: 0.9700 - val_loss: 4.5175 - val_acc: 0.5158\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.51682\n",
            "Epoch 156/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3427 - acc: 0.9699 - val_loss: 4.5127 - val_acc: 0.5165\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.51682\n",
            "Epoch 157/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3401 - acc: 0.9705 - val_loss: 4.4435 - val_acc: 0.5157\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.51682\n",
            "Epoch 158/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3451 - acc: 0.9693 - val_loss: 4.4725 - val_acc: 0.5162\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.51682\n",
            "Epoch 159/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3418 - acc: 0.9698 - val_loss: 4.5157 - val_acc: 0.5140\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.51682\n",
            "Epoch 160/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3420 - acc: 0.9695 - val_loss: 4.4678 - val_acc: 0.5147\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.51682\n",
            "Epoch 161/200\n",
            "Learning rate:  1e-05\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3404 - acc: 0.9695 - val_loss: 4.5414 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.51682\n",
            "Epoch 162/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3398 - acc: 0.9701 - val_loss: 4.5943 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.51682\n",
            "Epoch 163/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3422 - acc: 0.9682 - val_loss: 4.5439 - val_acc: 0.5131\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.51682\n",
            "Epoch 164/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3417 - acc: 0.9695 - val_loss: 4.5677 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.51682\n",
            "Epoch 165/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3421 - acc: 0.9691 - val_loss: 4.5336 - val_acc: 0.5126\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.51682\n",
            "Epoch 166/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3381 - acc: 0.9713 - val_loss: 4.5953 - val_acc: 0.5127\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.51682\n",
            "Epoch 167/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3400 - acc: 0.9695 - val_loss: 4.4977 - val_acc: 0.5123\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.51682\n",
            "Epoch 168/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3340 - acc: 0.9724 - val_loss: 4.5075 - val_acc: 0.5121\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.51682\n",
            "Epoch 169/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3360 - acc: 0.9716 - val_loss: 4.4606 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.51682\n",
            "Epoch 170/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3401 - acc: 0.9701 - val_loss: 4.5218 - val_acc: 0.5127\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.51682\n",
            "Epoch 171/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3377 - acc: 0.9720 - val_loss: 4.5098 - val_acc: 0.5134\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.51682\n",
            "Epoch 172/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3354 - acc: 0.9710 - val_loss: 4.4942 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.51682\n",
            "Epoch 173/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3410 - acc: 0.9703 - val_loss: 4.5090 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.51682\n",
            "Epoch 174/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3412 - acc: 0.9688 - val_loss: 4.5516 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.51682\n",
            "Epoch 175/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3354 - acc: 0.9704 - val_loss: 4.5544 - val_acc: 0.5147\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.51682\n",
            "Epoch 176/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 627ms/step - loss: 0.3357 - acc: 0.9719 - val_loss: 4.5683 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.51682\n",
            "Epoch 177/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3387 - acc: 0.9716 - val_loss: 4.4606 - val_acc: 0.5129\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.51682\n",
            "Epoch 178/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3356 - acc: 0.9724 - val_loss: 4.5326 - val_acc: 0.5131\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.51682\n",
            "Epoch 179/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3380 - acc: 0.9704 - val_loss: 4.5755 - val_acc: 0.5139\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.51682\n",
            "Epoch 180/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3401 - acc: 0.9701 - val_loss: 4.5128 - val_acc: 0.5142\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.51682\n",
            "Epoch 181/200\n",
            "Learning rate:  1e-06\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3397 - acc: 0.9700 - val_loss: 4.5044 - val_acc: 0.5139\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.51682\n",
            "Epoch 182/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3359 - acc: 0.9708 - val_loss: 4.5200 - val_acc: 0.5131\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.51682\n",
            "Epoch 183/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3392 - acc: 0.9694 - val_loss: 4.5232 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.51682\n",
            "Epoch 184/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3378 - acc: 0.9704 - val_loss: 4.5368 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.51682\n",
            "Epoch 185/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3347 - acc: 0.9714 - val_loss: 4.5321 - val_acc: 0.5139\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.51682\n",
            "Epoch 186/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3391 - acc: 0.9703 - val_loss: 4.5336 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.51682\n",
            "Epoch 187/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3335 - acc: 0.9723 - val_loss: 4.5504 - val_acc: 0.5142\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.51682\n",
            "Epoch 188/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3392 - acc: 0.9694 - val_loss: 4.5081 - val_acc: 0.5132\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.51682\n",
            "Epoch 189/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3374 - acc: 0.9705 - val_loss: 4.5105 - val_acc: 0.5149\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.51682\n",
            "Epoch 190/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3417 - acc: 0.9685 - val_loss: 4.5509 - val_acc: 0.5140\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.51682\n",
            "Epoch 191/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3333 - acc: 0.9707 - val_loss: 4.5630 - val_acc: 0.5142\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.51682\n",
            "Epoch 192/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3392 - acc: 0.9699 - val_loss: 4.4969 - val_acc: 0.5149\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.51682\n",
            "Epoch 193/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3360 - acc: 0.9715 - val_loss: 4.5025 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.51682\n",
            "Epoch 194/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3400 - acc: 0.9705 - val_loss: 4.4737 - val_acc: 0.5144\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.51682\n",
            "Epoch 195/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3350 - acc: 0.9706 - val_loss: 4.5479 - val_acc: 0.5147\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.51682\n",
            "Epoch 196/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3361 - acc: 0.9722 - val_loss: 4.5434 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.51682\n",
            "Epoch 197/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3398 - acc: 0.9699 - val_loss: 4.5095 - val_acc: 0.5144\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.51682\n",
            "Epoch 198/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3394 - acc: 0.9695 - val_loss: 4.4437 - val_acc: 0.5142\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.51682\n",
            "Epoch 199/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3384 - acc: 0.9717 - val_loss: 4.5098 - val_acc: 0.5142\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.51682\n",
            "Epoch 200/200\n",
            "Learning rate:  5e-07\n",
            "192/192 [==============================] - 121s 628ms/step - loss: 0.3352 - acc: 0.9719 - val_loss: 4.5791 - val_acc: 0.5137\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.51682\n",
            "Test loss: 4.579104423522949\n",
            "Test accuracy: 0.5137209892272949\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}